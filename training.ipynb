{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/training_module.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Topwear', 'Bottomwear', 'Footwear'], dtype=object)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "styles = get_df()\n",
    "\n",
    "styles[\"subCategory\"].unique() # we can check by this code that we only have three subcategory now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bottomwear', 'Footwear', 'Topwear'], dtype=object)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "le = LabelEncoder()\n",
    "#\n",
    "styles[\"subCategory\"] = le.fit_transform(styles[\"subCategory\"])\n",
    "\n",
    "styles.head()\n",
    "\n",
    "le.classes_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading image '/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/28492.jpg': Invalid shape\n",
      "Error reading image '/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/14776.jpg': Invalid shape\n",
      "Error reading image '/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/5408.jpg': Invalid shape\n",
      "Error reading image '/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/1799.jpg': Invalid shape\n",
      "Error reading image '/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/2311.jpg': Invalid shape\n",
      "Error reading image '/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/10284.jpg': Invalid shape\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " images (InputLayer)         [(None, 80, 60, 3)]       0         \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 3, 2, 2048)        23587712  \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 2, 1, 32)          262176    \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_297 (Dense)           (None, 1024)              66560     \n",
      "                                                                 \n",
      " dense_298 (Dense)           (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_299 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_300 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_301 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_302 (Dense)           (None, 3)                 195       \n",
      "                                                                 \n",
      " subCategory (Activation)    (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24613923 (93.89 MB)\n",
      "Trainable params: 1026211 (3.91 MB)\n",
      "Non-trainable params: 23587712 (89.98 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# styles, articleTypeLB, genderLB, baseColourLB, seasonLB, usageLB = my_le(styles)\n",
    "\n",
    "sub_train,sub_val,sub_test=make_input_xx(make_input_array_subcate(styles))\n",
    "sub_model = building_model(80,60)\n",
    "sub_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(9693) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(9694) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/environment/lib/python3.10/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 155s 76ms/step - loss: 0.2370 - accuracy: 0.9597 - val_loss: 0.0755 - val_accuracy: 0.9815\n",
      "Epoch 2/2\n",
      "2000/2000 [==============================] - 126s 63ms/step - loss: 0.0799 - accuracy: 0.9803 - val_loss: 0.0528 - val_accuracy: 0.9859\n",
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_sub/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_sub/assets\n",
      "/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/environment/lib/python3.10/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2727/2727 [==============================] - 76s 27ms/step - loss: 0.0573 - accuracy: 0.9851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05730447173118591, 0.9851458072662354]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(sub_model)\n",
    "\n",
    "sub_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "sub_history = sub_model.fit(sub_train, \n",
    "                    epochs=2, \n",
    "                    steps_per_epoch = 2000,\n",
    "                    validation_data = sub_val)\n",
    "\n",
    "sub_model.save(\"/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_sub\")\n",
    "\n",
    "test_model = tf.keras.models.load_model(\"/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_sub\")\n",
    "\n",
    "test_model.evaluate(sub_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_df = get_234_df(\"Topwear\")\n",
    "bottom_df = get_234_df(\"Bottomwear\")\n",
    "foot_df = get_234_df(\"Footwear\")\n",
    "top_df,top_art,top_gen,top_base,top_sea,top_usage = my_le(top_df)\n",
    "bottom_df,bottom_art,bottom_gen,bottom_base,bottom_sea,bottom_usage = my_le(bottom_df)\n",
    "foot_df,foot_art,foot_gen,foot_base,foot_sea,foot_usage = my_le(foot_df)\n",
    "foot_usage.classes_\n",
    "\n",
    "top_base_model = build_model(80,60,top_art,top_gen,top_base,top_sea,top_usage)\n",
    "bottom_base_model = build_model(80,60,bottom_art,bottom_gen,bottom_base,bottom_sea,bottom_usage)\n",
    "foot_base_model = build_model(80,60,foot_art,foot_gen,foot_base,foot_sea,foot_usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_train, top_val, top_test = make_input_xx(make_input_array_2(top_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_train, bottom_val, bottom_test = make_input_xx(make_input_array_2(bottom_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image 10284: Failed to load image at path: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/10284.jpg\n"
     ]
    }
   ],
   "source": [
    "foot_train, foot_val, foot_test = make_input_xx(make_input_array_2(foot_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image 10284: Failed to load image at path: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/Miniversion/smalldataset/images/10284.jpg\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/environment/lib/python3.10/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 81s 141ms/step - loss: 3.9789 - articleType_loss: 0.8899 - gender_loss: 0.3197 - baseColour_loss: 1.6051 - season_loss: 0.7496 - usage_loss: 0.4147 - articleType_accuracy: 0.7480 - gender_accuracy: 0.9200 - baseColour_accuracy: 0.4940 - season_accuracy: 0.6290 - usage_accuracy: 0.8710 - val_loss: 4.0296 - val_articleType_loss: 0.7949 - val_gender_loss: 0.2457 - val_baseColour_loss: 1.6225 - val_season_loss: 0.7891 - val_usage_loss: 0.5774 - val_articleType_accuracy: 0.7594 - val_gender_accuracy: 0.9275 - val_baseColour_accuracy: 0.5198 - val_season_accuracy: 0.5910 - val_usage_accuracy: 0.8739\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 4.0025 - articleType_loss: 0.8777 - gender_loss: 0.2778 - baseColour_loss: 1.6143 - season_loss: 0.7797 - usage_loss: 0.4530 - articleType_accuracy: 0.7460 - gender_accuracy: 0.9130 - baseColour_accuracy: 0.5010 - season_accuracy: 0.6150 - usage_accuracy: 0.8630 - val_loss: 3.9583 - val_articleType_loss: 0.8980 - val_gender_loss: 0.2068 - val_baseColour_loss: 1.5524 - val_season_loss: 0.7529 - val_usage_loss: 0.5482 - val_articleType_accuracy: 0.7285 - val_gender_accuracy: 0.9340 - val_baseColour_accuracy: 0.5208 - val_season_accuracy: 0.5917 - val_usage_accuracy: 0.8537\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 62s 123ms/step - loss: 4.1496 - articleType_loss: 0.9385 - gender_loss: 0.3319 - baseColour_loss: 1.6533 - season_loss: 0.7640 - usage_loss: 0.4619 - articleType_accuracy: 0.7450 - gender_accuracy: 0.9130 - baseColour_accuracy: 0.4940 - season_accuracy: 0.6090 - usage_accuracy: 0.8610 - val_loss: 4.0251 - val_articleType_loss: 0.7466 - val_gender_loss: 0.2693 - val_baseColour_loss: 1.4481 - val_season_loss: 1.1125 - val_usage_loss: 0.4484 - val_articleType_accuracy: 0.7789 - val_gender_accuracy: 0.9119 - val_baseColour_accuracy: 0.5614 - val_season_accuracy: 0.6421 - val_usage_accuracy: 0.8661\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 63s 125ms/step - loss: 3.7727 - articleType_loss: 0.7366 - gender_loss: 0.2718 - baseColour_loss: 1.5100 - season_loss: 0.8087 - usage_loss: 0.4457 - articleType_accuracy: 0.7670 - gender_accuracy: 0.9230 - baseColour_accuracy: 0.5370 - season_accuracy: 0.6060 - usage_accuracy: 0.8500 - val_loss: 3.5818 - val_articleType_loss: 0.7088 - val_gender_loss: 0.2167 - val_baseColour_loss: 1.5328 - val_season_loss: 0.7580 - val_usage_loss: 0.3655 - val_articleType_accuracy: 0.7858 - val_gender_accuracy: 0.9291 - val_baseColour_accuracy: 0.5348 - val_season_accuracy: 0.6525 - val_usage_accuracy: 0.8729\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 3.7989 - articleType_loss: 0.8245 - gender_loss: 0.2535 - baseColour_loss: 1.5512 - season_loss: 0.7779 - usage_loss: 0.3918 - articleType_accuracy: 0.7680 - gender_accuracy: 0.9140 - baseColour_accuracy: 0.5370 - season_accuracy: 0.6490 - usage_accuracy: 0.8720 - val_loss: 3.5008 - val_articleType_loss: 0.6580 - val_gender_loss: 0.2338 - val_baseColour_loss: 1.5086 - val_season_loss: 0.7394 - val_usage_loss: 0.3609 - val_articleType_accuracy: 0.7929 - val_gender_accuracy: 0.9314 - val_baseColour_accuracy: 0.5231 - val_season_accuracy: 0.6385 - val_usage_accuracy: 0.8774\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 62s 124ms/step - loss: 3.7873 - articleType_loss: 0.7820 - gender_loss: 0.2590 - baseColour_loss: 1.5794 - season_loss: 0.7673 - usage_loss: 0.3997 - articleType_accuracy: 0.7530 - gender_accuracy: 0.9400 - baseColour_accuracy: 0.5070 - season_accuracy: 0.6600 - usage_accuracy: 0.8820 - val_loss: 3.9110 - val_articleType_loss: 0.8491 - val_gender_loss: 0.3365 - val_baseColour_loss: 1.4755 - val_season_loss: 0.7276 - val_usage_loss: 0.5223 - val_articleType_accuracy: 0.7802 - val_gender_accuracy: 0.8976 - val_baseColour_accuracy: 0.5413 - val_season_accuracy: 0.6599 - val_usage_accuracy: 0.8235\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 3.7990 - articleType_loss: 0.7927 - gender_loss: 0.2616 - baseColour_loss: 1.5499 - season_loss: 0.7302 - usage_loss: 0.4646 - articleType_accuracy: 0.7640 - gender_accuracy: 0.9090 - baseColour_accuracy: 0.5230 - season_accuracy: 0.6570 - usage_accuracy: 0.8530 - val_loss: 3.7951 - val_articleType_loss: 0.7885 - val_gender_loss: 0.3232 - val_baseColour_loss: 1.4627 - val_season_loss: 0.7500 - val_usage_loss: 0.4707 - val_articleType_accuracy: 0.7851 - val_gender_accuracy: 0.9246 - val_baseColour_accuracy: 0.5566 - val_season_accuracy: 0.6502 - val_usage_accuracy: 0.8495\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 62s 123ms/step - loss: 3.8410 - articleType_loss: 0.7833 - gender_loss: 0.2597 - baseColour_loss: 1.5104 - season_loss: 0.8111 - usage_loss: 0.4765 - articleType_accuracy: 0.7880 - gender_accuracy: 0.9180 - baseColour_accuracy: 0.5350 - season_accuracy: 0.6130 - usage_accuracy: 0.8550 - val_loss: 3.8373 - val_articleType_loss: 0.8304 - val_gender_loss: 0.3148 - val_baseColour_loss: 1.4789 - val_season_loss: 0.7283 - val_usage_loss: 0.4849 - val_articleType_accuracy: 0.7643 - val_gender_accuracy: 0.9376 - val_baseColour_accuracy: 0.5302 - val_season_accuracy: 0.6486 - val_usage_accuracy: 0.8830\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 63s 125ms/step - loss: 3.5789 - articleType_loss: 0.7470 - gender_loss: 0.1986 - baseColour_loss: 1.4817 - season_loss: 0.7377 - usage_loss: 0.4139 - articleType_accuracy: 0.7750 - gender_accuracy: 0.9360 - baseColour_accuracy: 0.5450 - season_accuracy: 0.6410 - usage_accuracy: 0.8660 - val_loss: 3.5203 - val_articleType_loss: 0.7589 - val_gender_loss: 0.2093 - val_baseColour_loss: 1.3761 - val_season_loss: 0.7541 - val_usage_loss: 0.4220 - val_articleType_accuracy: 0.7793 - val_gender_accuracy: 0.9256 - val_baseColour_accuracy: 0.5686 - val_season_accuracy: 0.6476 - val_usage_accuracy: 0.8557\n",
      "Epoch 10/10\n",
      "115/500 [=====>........................] - ETA: 21s - loss: 3.6252 - articleType_loss: 0.6864 - gender_loss: 0.2844 - baseColour_loss: 1.5262 - season_loss: 0.6892 - usage_loss: 0.4390 - articleType_accuracy: 0.7904 - gender_accuracy: 0.9258 - baseColour_accuracy: 0.5022 - season_accuracy: 0.6638 - usage_accuracy: 0.8559WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 5000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 5000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 43s 86ms/step - loss: 3.6252 - articleType_loss: 0.6864 - gender_loss: 0.2844 - baseColour_loss: 1.5262 - season_loss: 0.6892 - usage_loss: 0.4390 - articleType_accuracy: 0.7904 - gender_accuracy: 0.9258 - baseColour_accuracy: 0.5022 - season_accuracy: 0.6638 - usage_accuracy: 0.8559 - val_loss: 3.6830 - val_articleType_loss: 0.9566 - val_gender_loss: 0.2309 - val_baseColour_loss: 1.4278 - val_season_loss: 0.7057 - val_usage_loss: 0.3620 - val_articleType_accuracy: 0.7542 - val_gender_accuracy: 0.9330 - val_baseColour_accuracy: 0.5621 - val_season_accuracy: 0.6736 - val_usage_accuracy: 0.8859\n",
      "1539/1539 [==============================] - 42s 27ms/step - loss: 3.7436 - articleType_loss: 0.9520 - gender_loss: 0.2681 - baseColour_loss: 1.4081 - season_loss: 0.7328 - usage_loss: 0.3827 - articleType_accuracy: 0.7612 - gender_accuracy: 0.9292 - baseColour_accuracy: 0.5513 - season_accuracy: 0.6417 - usage_accuracy: 0.8840\n",
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_top/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_top/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/environment/lib/python3.10/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 14s 208ms/step - loss: 5.9505 - articleType_loss: 1.4084 - gender_loss: 0.9655 - baseColour_loss: 1.8132 - season_loss: 1.0292 - usage_loss: 0.7341 - articleType_accuracy: 0.5300 - gender_accuracy: 0.7200 - baseColour_accuracy: 0.3300 - season_accuracy: 0.5000 - usage_accuracy: 0.7600 - val_loss: 5.7967 - val_articleType_loss: 1.3769 - val_gender_loss: 0.5981 - val_baseColour_loss: 1.7296 - val_season_loss: 0.9814 - val_usage_loss: 1.1108 - val_articleType_accuracy: 0.5493 - val_gender_accuracy: 0.7579 - val_baseColour_accuracy: 0.4264 - val_season_accuracy: 0.5233 - val_usage_accuracy: 0.7337\n",
      "Epoch 2/15\n",
      "50/50 [==============================] - 10s 195ms/step - loss: 5.7425 - articleType_loss: 1.4864 - gender_loss: 0.5633 - baseColour_loss: 1.8357 - season_loss: 1.0517 - usage_loss: 0.8053 - articleType_accuracy: 0.6300 - gender_accuracy: 0.7400 - baseColour_accuracy: 0.3900 - season_accuracy: 0.5600 - usage_accuracy: 0.7700 - val_loss: 6.0538 - val_articleType_loss: 1.6435 - val_gender_loss: 0.5819 - val_baseColour_loss: 2.0940 - val_season_loss: 0.8507 - val_usage_loss: 0.8837 - val_articleType_accuracy: 0.5754 - val_gender_accuracy: 0.8268 - val_baseColour_accuracy: 0.3296 - val_season_accuracy: 0.6034 - val_usage_accuracy: 0.6927\n",
      "Epoch 3/15\n",
      "50/50 [==============================] - 9s 177ms/step - loss: 5.6183 - articleType_loss: 1.3211 - gender_loss: 0.7834 - baseColour_loss: 1.9213 - season_loss: 0.8253 - usage_loss: 0.7672 - articleType_accuracy: 0.6100 - gender_accuracy: 0.6900 - baseColour_accuracy: 0.3900 - season_accuracy: 0.6600 - usage_accuracy: 0.7400 - val_loss: 5.4302 - val_articleType_loss: 1.2533 - val_gender_loss: 0.7192 - val_baseColour_loss: 1.6979 - val_season_loss: 0.8978 - val_usage_loss: 0.8619 - val_articleType_accuracy: 0.6164 - val_gender_accuracy: 0.6648 - val_baseColour_accuracy: 0.4451 - val_season_accuracy: 0.5531 - val_usage_accuracy: 0.6872\n",
      "Epoch 4/15\n",
      "50/50 [==============================] - 9s 181ms/step - loss: 5.7597 - articleType_loss: 1.3002 - gender_loss: 0.6784 - baseColour_loss: 1.8874 - season_loss: 1.1026 - usage_loss: 0.7911 - articleType_accuracy: 0.6200 - gender_accuracy: 0.6900 - baseColour_accuracy: 0.3400 - season_accuracy: 0.5500 - usage_accuracy: 0.6800 - val_loss: 5.6167 - val_articleType_loss: 1.5655 - val_gender_loss: 0.5529 - val_baseColour_loss: 1.7893 - val_season_loss: 0.9455 - val_usage_loss: 0.7634 - val_articleType_accuracy: 0.6201 - val_gender_accuracy: 0.7840 - val_baseColour_accuracy: 0.4041 - val_season_accuracy: 0.4581 - val_usage_accuracy: 0.6872\n",
      "Epoch 5/15\n",
      "50/50 [==============================] - 10s 190ms/step - loss: 5.4449 - articleType_loss: 1.3830 - gender_loss: 0.5902 - baseColour_loss: 1.8330 - season_loss: 0.9720 - usage_loss: 0.6667 - articleType_accuracy: 0.5500 - gender_accuracy: 0.7600 - baseColour_accuracy: 0.3800 - season_accuracy: 0.5300 - usage_accuracy: 0.7400 - val_loss: 4.8408 - val_articleType_loss: 1.2604 - val_gender_loss: 0.4344 - val_baseColour_loss: 1.6634 - val_season_loss: 0.8731 - val_usage_loss: 0.6095 - val_articleType_accuracy: 0.5791 - val_gender_accuracy: 0.8305 - val_baseColour_accuracy: 0.4711 - val_season_accuracy: 0.5866 - val_usage_accuracy: 0.7374\n",
      "Epoch 6/15\n",
      "50/50 [==============================] - 9s 185ms/step - loss: 5.3688 - articleType_loss: 1.3751 - gender_loss: 0.6063 - baseColour_loss: 1.8562 - season_loss: 0.8896 - usage_loss: 0.6417 - articleType_accuracy: 0.5100 - gender_accuracy: 0.7300 - baseColour_accuracy: 0.4600 - season_accuracy: 0.5900 - usage_accuracy: 0.7200 - val_loss: 5.3178 - val_articleType_loss: 1.3766 - val_gender_loss: 0.5531 - val_baseColour_loss: 1.7352 - val_season_loss: 0.9738 - val_usage_loss: 0.6792 - val_articleType_accuracy: 0.6015 - val_gender_accuracy: 0.7821 - val_baseColour_accuracy: 0.4786 - val_season_accuracy: 0.6182 - val_usage_accuracy: 0.7281\n",
      "Epoch 7/15\n",
      "50/50 [==============================] - 9s 182ms/step - loss: 4.9978 - articleType_loss: 1.1813 - gender_loss: 0.6369 - baseColour_loss: 1.6031 - season_loss: 0.8512 - usage_loss: 0.7253 - articleType_accuracy: 0.6800 - gender_accuracy: 0.7500 - baseColour_accuracy: 0.5100 - season_accuracy: 0.6700 - usage_accuracy: 0.7200 - val_loss: 4.9537 - val_articleType_loss: 1.1497 - val_gender_loss: 0.5225 - val_baseColour_loss: 1.6075 - val_season_loss: 0.9721 - val_usage_loss: 0.7018 - val_articleType_accuracy: 0.6574 - val_gender_accuracy: 0.8231 - val_baseColour_accuracy: 0.4898 - val_season_accuracy: 0.5680 - val_usage_accuracy: 0.7486\n",
      "Epoch 8/15\n",
      "50/50 [==============================] - 9s 181ms/step - loss: 5.3945 - articleType_loss: 1.4808 - gender_loss: 0.5639 - baseColour_loss: 1.8570 - season_loss: 0.7953 - usage_loss: 0.6974 - articleType_accuracy: 0.4900 - gender_accuracy: 0.7800 - baseColour_accuracy: 0.3700 - season_accuracy: 0.6300 - usage_accuracy: 0.7200 - val_loss: 5.1450 - val_articleType_loss: 1.3686 - val_gender_loss: 0.4926 - val_baseColour_loss: 1.8956 - val_season_loss: 0.8216 - val_usage_loss: 0.5665 - val_articleType_accuracy: 0.5661 - val_gender_accuracy: 0.8026 - val_baseColour_accuracy: 0.3408 - val_season_accuracy: 0.5568 - val_usage_accuracy: 0.7877\n",
      "Epoch 9/15\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 5.7918 - articleType_loss: 1.6381 - gender_loss: 0.6609 - baseColour_loss: 1.7541 - season_loss: 0.9444 - usage_loss: 0.7943 - articleType_accuracy: 0.5400 - gender_accuracy: 0.7400 - baseColour_accuracy: 0.4000 - season_accuracy: 0.5700 - usage_accuracy: 0.7100 - val_loss: 5.0828 - val_articleType_loss: 1.3144 - val_gender_loss: 0.5345 - val_baseColour_loss: 1.6747 - val_season_loss: 0.8979 - val_usage_loss: 0.6613 - val_articleType_accuracy: 0.6145 - val_gender_accuracy: 0.8063 - val_baseColour_accuracy: 0.4842 - val_season_accuracy: 0.5847 - val_usage_accuracy: 0.7300\n",
      "Epoch 10/15\n",
      "50/50 [==============================] - 9s 176ms/step - loss: 5.6256 - articleType_loss: 1.6355 - gender_loss: 0.7836 - baseColour_loss: 1.6838 - season_loss: 0.9194 - usage_loss: 0.6032 - articleType_accuracy: 0.5300 - gender_accuracy: 0.6800 - baseColour_accuracy: 0.4700 - season_accuracy: 0.5300 - usage_accuracy: 0.7600 - val_loss: 5.8234 - val_articleType_loss: 1.1504 - val_gender_loss: 0.5175 - val_baseColour_loss: 2.1042 - val_season_loss: 0.8293 - val_usage_loss: 1.2220 - val_articleType_accuracy: 0.6667 - val_gender_accuracy: 0.7765 - val_baseColour_accuracy: 0.3669 - val_season_accuracy: 0.6089 - val_usage_accuracy: 0.7300\n",
      "Epoch 11/15\n",
      "50/50 [==============================] - 9s 182ms/step - loss: 4.9778 - articleType_loss: 1.1182 - gender_loss: 0.4587 - baseColour_loss: 1.6410 - season_loss: 0.9999 - usage_loss: 0.7600 - articleType_accuracy: 0.7300 - gender_accuracy: 0.8300 - baseColour_accuracy: 0.5100 - season_accuracy: 0.5600 - usage_accuracy: 0.7400 - val_loss: 5.2989 - val_articleType_loss: 1.3291 - val_gender_loss: 0.6287 - val_baseColour_loss: 1.9318 - val_season_loss: 0.8417 - val_usage_loss: 0.5677 - val_articleType_accuracy: 0.6257 - val_gender_accuracy: 0.7430 - val_baseColour_accuracy: 0.4171 - val_season_accuracy: 0.6182 - val_usage_accuracy: 0.7654\n",
      "Epoch 12/15\n",
      "50/50 [==============================] - 8s 171ms/step - loss: 5.2353 - articleType_loss: 1.2403 - gender_loss: 0.5188 - baseColour_loss: 1.8767 - season_loss: 0.8160 - usage_loss: 0.7834 - articleType_accuracy: 0.5900 - gender_accuracy: 0.7700 - baseColour_accuracy: 0.4000 - season_accuracy: 0.5500 - usage_accuracy: 0.7000 - val_loss: 5.3085 - val_articleType_loss: 1.4678 - val_gender_loss: 0.5203 - val_baseColour_loss: 1.7287 - val_season_loss: 0.8767 - val_usage_loss: 0.7150 - val_articleType_accuracy: 0.5680 - val_gender_accuracy: 0.8007 - val_baseColour_accuracy: 0.4097 - val_season_accuracy: 0.5698 - val_usage_accuracy: 0.7356\n",
      "Epoch 13/15\n",
      "50/50 [==============================] - 10s 199ms/step - loss: 5.1741 - articleType_loss: 1.3667 - gender_loss: 0.4767 - baseColour_loss: 1.7543 - season_loss: 0.7871 - usage_loss: 0.7893 - articleType_accuracy: 0.5800 - gender_accuracy: 0.8200 - baseColour_accuracy: 0.4100 - season_accuracy: 0.5900 - usage_accuracy: 0.7200 - val_loss: 4.8852 - val_articleType_loss: 1.1670 - val_gender_loss: 0.4525 - val_baseColour_loss: 1.7640 - val_season_loss: 0.8449 - val_usage_loss: 0.6567 - val_articleType_accuracy: 0.6406 - val_gender_accuracy: 0.8194 - val_baseColour_accuracy: 0.4320 - val_season_accuracy: 0.6350 - val_usage_accuracy: 0.7393\n",
      "Epoch 14/15\n",
      "50/50 [==============================] - 10s 202ms/step - loss: 4.7113 - articleType_loss: 1.3343 - gender_loss: 0.3251 - baseColour_loss: 1.6815 - season_loss: 0.7186 - usage_loss: 0.6518 - articleType_accuracy: 0.6100 - gender_accuracy: 0.8900 - baseColour_accuracy: 0.4800 - season_accuracy: 0.6900 - usage_accuracy: 0.7400 - val_loss: 5.1751 - val_articleType_loss: 1.2580 - val_gender_loss: 0.5736 - val_baseColour_loss: 1.7364 - val_season_loss: 1.0817 - val_usage_loss: 0.5254 - val_articleType_accuracy: 0.6182 - val_gender_accuracy: 0.7728 - val_baseColour_accuracy: 0.4358 - val_season_accuracy: 0.5996 - val_usage_accuracy: 0.7747\n",
      "Epoch 15/15\n",
      "50/50 [==============================] - 10s 193ms/step - loss: 5.5206 - articleType_loss: 1.2956 - gender_loss: 0.6961 - baseColour_loss: 1.7927 - season_loss: 1.0387 - usage_loss: 0.6975 - articleType_accuracy: 0.5800 - gender_accuracy: 0.7400 - baseColour_accuracy: 0.4100 - season_accuracy: 0.6300 - usage_accuracy: 0.7100 - val_loss: 4.8779 - val_articleType_loss: 1.2502 - val_gender_loss: 0.5129 - val_baseColour_loss: 1.5457 - val_season_loss: 0.9410 - val_usage_loss: 0.6282 - val_articleType_accuracy: 0.5680 - val_gender_accuracy: 0.7803 - val_baseColour_accuracy: 0.4655 - val_season_accuracy: 0.6108 - val_usage_accuracy: 0.7654\n",
      "269/269 [==============================] - 7s 27ms/step - loss: 4.8872 - articleType_loss: 1.2202 - gender_loss: 0.5383 - baseColour_loss: 1.5799 - season_loss: 0.9086 - usage_loss: 0.6403 - articleType_accuracy: 0.5959 - gender_accuracy: 0.7672 - baseColour_accuracy: 0.4786 - season_accuracy: 0.5903 - usage_accuracy: 0.7709\n",
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_bottom/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_bottom/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/environment/lib/python3.10/site-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 216s 107ms/step - loss: 4.6747 - articleType_loss: 0.9428 - gender_loss: 0.5877 - baseColour_loss: 1.5342 - season_loss: 1.0550 - usage_loss: 0.5550 - articleType_accuracy: 0.6700 - gender_accuracy: 0.8133 - baseColour_accuracy: 0.5195 - season_accuracy: 0.5455 - usage_accuracy: 0.8257 - val_loss: 4.3156 - val_articleType_loss: 0.8118 - val_gender_loss: 0.5385 - val_baseColour_loss: 1.4754 - val_season_loss: 1.0425 - val_usage_loss: 0.4473 - val_articleType_accuracy: 0.6966 - val_gender_accuracy: 0.8249 - val_baseColour_accuracy: 0.5329 - val_season_accuracy: 0.5443 - val_usage_accuracy: 0.8526\n",
      "Epoch 2/5\n",
      " 759/2000 [==========>...................] - ETA: 1:19 - loss: 4.4320 - articleType_loss: 0.8889 - gender_loss: 0.5676 - baseColour_loss: 1.4509 - season_loss: 1.0482 - usage_loss: 0.4763 - articleType_accuracy: 0.6937 - gender_accuracy: 0.8307 - baseColour_accuracy: 0.5573 - season_accuracy: 0.5435 - usage_accuracy: 0.8412WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 76s 38ms/step - loss: 4.4320 - articleType_loss: 0.8889 - gender_loss: 0.5676 - baseColour_loss: 1.4509 - season_loss: 1.0482 - usage_loss: 0.4763 - articleType_accuracy: 0.6937 - gender_accuracy: 0.8307 - baseColour_accuracy: 0.5573 - season_accuracy: 0.5435 - usage_accuracy: 0.8412 - val_loss: 4.2046 - val_articleType_loss: 0.7100 - val_gender_loss: 0.5111 - val_baseColour_loss: 1.5186 - val_season_loss: 1.0090 - val_usage_loss: 0.4559 - val_articleType_accuracy: 0.7455 - val_gender_accuracy: 0.8070 - val_baseColour_accuracy: 0.5220 - val_season_accuracy: 0.5568 - val_usage_accuracy: 0.8630\n",
      "920/920 [==============================] - 31s 33ms/step - loss: 4.0634 - articleType_loss: 0.7126 - gender_loss: 0.4708 - baseColour_loss: 1.4727 - season_loss: 1.0074 - usage_loss: 0.3999 - articleType_accuracy: 0.7451 - gender_accuracy: 0.8217 - baseColour_accuracy: 0.5348 - season_accuracy: 0.5478 - usage_accuracy: 0.8603\n",
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_foot/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_foot/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "foot_train, foot_val, foot_test = make_input_xx(make_input_array_2(foot_df))\n",
    "\n",
    "top_base_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "bottom_base_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "foot_base_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "top_history = top_base_model.fit(top_train, \n",
    "                    epochs=10, \n",
    "                    steps_per_epoch = 500,\n",
    "                    validation_data = top_val)\n",
    "\n",
    "top_base_model.evaluate(top_test)\n",
    "\n",
    "top_base_model.save(\"/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_top\")\n",
    "\n",
    "\n",
    "\n",
    "bottom_history = bottom_base_model.fit(bottom_train, \n",
    "                    epochs=15, \n",
    "                    steps_per_epoch = 50,\n",
    "                    validation_data = bottom_val)\n",
    "\n",
    "bottom_base_model.evaluate(bottom_test)\n",
    "\n",
    "bottom_base_model.save(\"/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_bottom\")\n",
    "\n",
    "\n",
    "\n",
    "foot_history = foot_base_model.fit(foot_train, \n",
    "                    epochs=5, \n",
    "                    steps_per_epoch = 2000,\n",
    "                    validation_data = foot_val)\n",
    "\n",
    "foot_base_model.evaluate(foot_test)\n",
    "\n",
    "foot_base_model.save(\"/Users/rijanbhandari/COLLEGE/sixthsem/minorproject/Pahiran/RESTART/model/model_foot\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
